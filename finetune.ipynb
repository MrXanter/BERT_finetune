{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba1291ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325b0da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e1b7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train files: 4\n",
      "Eval files: 1\n",
      "Train data: 887686\n",
      "Evaluation data: 206725\n",
      "Train dataset saved to 'train_dataset.csv'\n",
      "Evaluation dataset saved to 'eval_dataset.csv'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Get list of all csv files, excluding \"product_info.csv\"\n",
    "csv_dir = os.getcwd()  # Current working directory is the same as where the notebook is located\n",
    "csv_files = [os.path.join(csv_dir, f) for f in os.listdir(csv_dir) if f.endswith(\".csv\") and f != \"product_info.csv\"]\n",
    "\n",
    "# Combine all csv files except \"product_info.csv\"\n",
    "all_csv_files = csv_files\n",
    "\n",
    "# Shuffle files and split (95% train, 5% eval)\n",
    "train_files, eval_files = train_test_split(all_csv_files, test_size=0.05, random_state=42)\n",
    "\n",
    "print(f\"Train files found: {len(train_files)}\")\n",
    "\n",
    "# Function to read all content from a list of csv files\n",
    "def read_csv_files(file_list):\n",
    "    data = []\n",
    "    for file_path in file_list:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                reader = csv.DictReader(f)\n",
    "                data.extend(list(reader))\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "    return data\n",
    "\n",
    "# Read train and eval data\n",
    "train_data = read_csv_files(train_files)\n",
    "eval_data = read_csv_files(eval_files)\n",
    "\n",
    "print(f\"Train data: {len(train_data)}\")\n",
    "print(f\"Evaluation data: {len(eval_data)}\")\n",
    "\n",
    "# Create a simple dataset object (you may need to adapt this based on your specific requirements)\n",
    "class CSVDataset:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "train_dataset = CSVDataset(train_data)\n",
    "eval_dataset = CSVDataset(eval_data)\n",
    "\n",
    "# Optionally, if you need to save the datasets to files\n",
    "import pandas as pd\n",
    "\n",
    "# Save train dataset to csv file\n",
    "df_train = pd.DataFrame(train_data)\n",
    "df_train.to_csv('train_dataset.csv', index=False)\n",
    "\n",
    "print(\"Train dataset saved to 'train_dataset.csv'\")\n",
    "\n",
    "# Save eval dataset to csv file\n",
    "df_eval = pd.DataFrame(eval_data)\n",
    "df_eval.to_csv('eval_dataset.csv', index=False)\n",
    "\n",
    "print(\"Evaluation dataset saved to 'eval_dataset.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f721919c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total # of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset= eval_dataset            # evaluation dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de76ebab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.train()\n",
    "#trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5f40bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
