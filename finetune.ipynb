{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1291ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea8588ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.8.0-cp314-cp314-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.24.1 in ./.venv/lib/python3.14/site-packages (from scikit-learn) (2.3.5)\n",
      "Collecting scipy>=1.10.0 (from scikit-learn)\n",
      "  Downloading scipy-1.17.0-cp314-cp314-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Collecting joblib>=1.3.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting threadpoolctl>=3.2.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.8.0-cp314-cp314-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (8.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.3-py3-none-any.whl (309 kB)\n",
      "Downloading scipy-1.17.0-cp314-cp314-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (35.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m35.0/35.0 MB\u001b[0m \u001b[31m82.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4/4\u001b[0m [scikit-learn][0m [scikit-learn]\n",
      "\u001b[1A\u001b[2KSuccessfully installed joblib-1.5.3 scikit-learn-1.8.0 scipy-1.17.0 threadpoolctl-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325b0da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5e1b7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train files: 162\n",
      "Eval files: 9\n",
      "Train texts: 882759\n",
      "Eval texts: 50132\n",
      "Datasets saved to 'train_dataset.txt' and 'eval_dataset.txt'\n"
     ]
    }
   ],
   "source": [
    "#dividing datasets\n",
    "import os\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Get list of all wiki files\n",
    "wiki_dir_1 = \"wikipedia_data/1of2\"\n",
    "wiki_dir_2 = \"wikipedia_data/2of2\"\n",
    "\n",
    "wiki_files_1 = [os.path.join(wiki_dir_1, f) for f in os.listdir(wiki_dir_1) if f.startswith(\"wiki_\")]\n",
    "wiki_files_2 = [os.path.join(wiki_dir_2, f) for f in os.listdir(wiki_dir_2) if f.startswith(\"wiki_\")]\n",
    "\n",
    "# Combine all wiki files\n",
    "all_wiki_files = wiki_files_1 + wiki_files_2\n",
    "\n",
    "# Shuffle files and split (95% train, 5% eval)\n",
    "train_files, eval_files = train_test_split(all_wiki_files, test_size=0.05, random_state=42)\n",
    "\n",
    "print(f\"Train files: {len(train_files)}\")\n",
    "print(f\"Eval files: {len(eval_files)}\")\n",
    "\n",
    "# Function to read all content from a list of files\n",
    "def read_files(file_list):\n",
    "    texts = []\n",
    "    for file_path in file_list:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "                # Split by double newlines to separate articles\n",
    "                articles = content.split('\\n\\n')\n",
    "                # Filter out empty articles\n",
    "                articles = [article.strip() for article in articles if article.strip()]\n",
    "                texts.extend(articles)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "    return texts\n",
    "\n",
    "# Read train and eval data\n",
    "train_texts = read_files(train_files)\n",
    "eval_texts = read_files(eval_files)\n",
    "\n",
    "print(f\"Train texts: {len(train_texts)}\")\n",
    "print(f\"Eval texts: {len(eval_texts)}\")\n",
    "\n",
    "# Save train and eval datasets to text files\n",
    "with open('train_dataset.txt', 'w', encoding='utf-8') as f:\n",
    "    for text in train_texts:\n",
    "        f.write(text + '\\n\\n')\n",
    "\n",
    "with open('eval_dataset.txt', 'w', encoding='utf-8') as f:\n",
    "    for text in eval_texts:\n",
    "        f.write(text + '\\n\\n')\n",
    "\n",
    "print(\"Datasets saved to 'train_dataset.txt' and 'eval_dataset.txt'\")\n",
    "\n",
    "# Create simple dataset objects (you may need to adapt this based on your specific requirements)\n",
    "class TextDataset:\n",
    "    def __init__(self, texts):\n",
    "        self.texts = texts\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx]\n",
    "\n",
    "train_dataset = TextDataset(train_texts)\n",
    "eval_dataset = TextDataset(eval_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f721919c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total # of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ğŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset= eval_dataset            # evaluation dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de76ebab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.train()\n",
    "#trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5f40bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
